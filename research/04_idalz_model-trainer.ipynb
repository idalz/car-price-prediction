{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\DataScienceProjects\\\\car-price-prediction'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('..')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainerConfig:\n",
    "    root_dir: Path\n",
    "    processed_dataset_dir: Path\n",
    "    embed_dim_file_path: str\n",
    "    num_dim_file_path: str\n",
    "    model_dir: Path\n",
    "    batch_size: int\n",
    "    epochs: int\n",
    "    lr: float\n",
    "    layers: List\n",
    "    dropout: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Manager\n",
    "from carPricePrediction.constants import *\n",
    "from carPricePrediction.utils.common import read_yaml, create_directories\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "            self,\n",
    "            config_filepath = CONFIG_FILE_PATH,\n",
    "            params_filepath = PARAMS_FILE_PATH\n",
    "    ):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_model_trainer_config(self)-> ModelTrainerConfig:\n",
    "        config = self.config.model_trainer\n",
    "        params = self.params.TrainingArguments\n",
    "\n",
    "        create_directories([\n",
    "            config.root_dir, \n",
    "            config.model_dir\n",
    "        ])\n",
    "\n",
    "        data_transformation_config = ModelTrainerConfig(\n",
    "            root_dir = config.root_dir,\n",
    "            processed_dataset_dir = config.processed_dataset_dir,\n",
    "            embed_dim_file_path = config.embed_dim_file_path,\n",
    "            num_dim_file_path = config.num_dim_file_path,\n",
    "            model_dir = config.model_dir,\n",
    "            batch_size = params.batch_size,\n",
    "            epochs = params.epochs,\n",
    "            lr = params.lr,\n",
    "            layers = params.layers,\n",
    "            dropout = params.dropout       \n",
    "        )\n",
    "\n",
    "        return data_transformation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model class\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FeedForwardNN(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, n_cont, out_dim, layers, dout=0.5):\n",
    "        super().__init__()\n",
    "        # Create embedding layer\n",
    "        self.embeds = nn.ModuleList([nn.Embedding(inp, out) for inp,out in embedding_dim])\n",
    "        # Apply dropout (prevent overfitting)\n",
    "        self.emb_drop = nn.Dropout(dout)\n",
    "        # Apply batch normalization to numerical features\n",
    "        self.bn_cont = nn.BatchNorm1d(n_cont)\n",
    "\n",
    "        # Total dimension of embedding layers\n",
    "        n_emb = sum((out for inp,out in embedding_dim))\n",
    "        # Total input (embedding and continuous) \n",
    "        n_inp = n_emb + n_cont\n",
    "\n",
    "        layerlist = []\n",
    "        for i in layers:\n",
    "            # Create Linear layer\n",
    "            layerlist.append(nn.Linear(n_inp,i))\n",
    "            # Add activation function\n",
    "            layerlist.append(nn.ReLU(inplace=True))\n",
    "            # Add batch normalization in the neurons\n",
    "            layerlist.append(nn.BatchNorm1d(i))\n",
    "            # Drop some neurons\n",
    "            layerlist.append(nn.Dropout(dout))\n",
    "            # The input for the next layer\n",
    "            n_inp = i\n",
    "        # Final layer - output layer\n",
    "        layerlist.append(nn.Linear(layers[-1], out_dim))\n",
    "\n",
    "        # Wrap the layers with Sequential\n",
    "        self.layers = nn.Sequential(*layerlist)\n",
    "\n",
    "    def forward(self, x_cat, x_cont): \n",
    "        # Create and concat embeddings\n",
    "        embeddings = []\n",
    "        for i,e in enumerate(self.embeds):\n",
    "            embeddings.append(e(x_cat[:,i]))\n",
    "        x_cat = torch.cat(embeddings, 1)\n",
    "        # Dropout\n",
    "        x_cat = self.emb_drop(x_cat)\n",
    "        # Batch normalization\n",
    "        x_cont = self.bn_cont(x_cont)\n",
    "        # Concat all features\n",
    "        x = torch.cat([x_cat, x_cont], 1)\n",
    "        # Return layer with input x\n",
    "        layer = self.layers(x)\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from carPricePrediction.components.feed_forward_nn import FeedForwardNN\n",
    "from carPricePrediction.entity import ModelTrainerConfig\n",
    "from carPricePrediction.logging.model_logger import ModelLogger\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, config: ModelTrainerConfig):\n",
    "        self.config = config\n",
    "    \n",
    "    def get_shuffled_batches(self, train_data, val_data, batch_size):\n",
    "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "        return train_loader, val_loader\n",
    "\n",
    "    def train(self):\n",
    "        # Choose device\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "        # Load train and validation data\n",
    "        train_filepath = os.path.join(self.config.processed_dataset_dir, \"train.pth\")\n",
    "        train_data = torch.load(train_filepath)\n",
    "\n",
    "        val_filepath = os.path.join(self.config.processed_dataset_dir, \"val.pth\")\n",
    "        val_data = torch.load(val_filepath)\n",
    "\n",
    "        # Load dimensions\n",
    "        with open(self.config.num_dim_file_path, 'rb') as f:\n",
    "            num_dim = pickle.load(f)\n",
    "\n",
    "        with open(self.config.embed_dim_file_path, 'rb') as f:\n",
    "            embedding_dim = pickle.load(f)\n",
    "        \n",
    "        # Set batches\n",
    "        batch_size = self.config.batch_size\n",
    "        train_loader, val_loader = self.get_shuffled_batches(train_data, val_data, batch_size)\n",
    "        \n",
    "        # Create model\n",
    "        torch.manual_seed(100)\n",
    "        model = FeedForwardNN(\n",
    "            embedding_dim=embedding_dim, \n",
    "            n_cont=num_dim, \n",
    "            out_dim=1, \n",
    "            layers=self.config.layers, \n",
    "            dout=self.config.dropout\n",
    "        ).to(device)\n",
    "\n",
    "        # Choose loss funciton and optimizer\n",
    "        loss_function = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=self.config.lr)\n",
    "\n",
    "        # Create the model logger\n",
    "        model_logger_file_path = os.path.join(self.config.model_dir, 'trainfile.log')\n",
    "        modelLogger = ModelLogger(model_logger_file_path)\n",
    "\n",
    "        # Set epochs\n",
    "        num_epochs = self.config.epochs \n",
    "\n",
    "        # Train model\n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0\n",
    "            model.train()\n",
    "            for batch_idx, (cat_data, num_data, target) in enumerate(train_loader):\n",
    "                cat_data, num_data, target = cat_data.to(device), num_data.to(device), target.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                prediction = model(cat_data, num_data)\n",
    "                loss = torch.sqrt(loss_function(prediction, target))\n",
    "                total_loss += loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            total_loss /= len(train_loader)\n",
    "            \n",
    "            # Get validation loss\n",
    "            val_loss = 0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for cat_data, num_data, target in val_loader:\n",
    "                    cat_data, num_data, target = cat_data.to(device), num_data.to(device), target.to(device)\n",
    "                    prediction = model(cat_data, num_data)\n",
    "                    val_loss += torch.sqrt(loss_function(prediction, target))\n",
    "                val_loss /= len(val_loader)\n",
    "\n",
    "            # Add to log file\n",
    "            message = f'Epoch {epoch+1}/{num_epochs}, train_Loss: {total_loss}, val_Loss: {val_loss:.4f}'\n",
    "            modelLogger.log_message(message)\n",
    "\n",
    "        # Save the entire model\n",
    "        model_file_path = os.path.join(self.config.model_dir, 'model.pth')\n",
    "        torch.save(model, model_file_path)\n",
    "\n",
    "        # Save model's state\n",
    "        model_state_file_path = os.path.join(self.config.model_dir, 'model_state_dict.pth')\n",
    "        torch.save(model.state_dict(), model_state_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-30 19:37:02,136: INFO: common: yaml file: config\\config.yaml loaded successfully.]\n",
      "[2024-03-30 19:37:02,139: INFO: common: yaml file: params.yaml loaded successfully.]\n",
      "[2024-03-30 19:37:02,141: INFO: common: Created directory at: artifacts]\n",
      "[2024-03-30 19:37:02,142: INFO: common: Created directory at: artifacts/model]\n",
      "[2024-03-30 19:37:02,143: INFO: common: Created directory at: artifacts/model]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\idale\\anaconda3\\envs\\gpuenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-30 19:37:04,265: INFO: model_logger: Epoch 1/100, train_Loss: 23586.033203125, val_Loss: 23362.4336]\n",
      "[2024-03-30 19:37:04,501: INFO: model_logger: Epoch 2/100, train_Loss: 23473.0390625, val_Loss: 24241.3516]\n",
      "[2024-03-30 19:37:04,814: INFO: model_logger: Epoch 3/100, train_Loss: 23535.814453125, val_Loss: 24000.9824]\n",
      "[2024-03-30 19:37:05,035: INFO: model_logger: Epoch 4/100, train_Loss: 23487.115234375, val_Loss: 23438.4258]\n",
      "[2024-03-30 19:37:05,251: INFO: model_logger: Epoch 5/100, train_Loss: 23486.041015625, val_Loss: 23904.2480]\n",
      "[2024-03-30 19:37:05,523: INFO: model_logger: Epoch 6/100, train_Loss: 23478.46484375, val_Loss: 23172.3984]\n",
      "[2024-03-30 19:37:05,724: INFO: model_logger: Epoch 7/100, train_Loss: 23389.880859375, val_Loss: 23328.5117]\n",
      "[2024-03-30 19:37:05,930: INFO: model_logger: Epoch 8/100, train_Loss: 23430.193359375, val_Loss: 23653.4375]\n",
      "[2024-03-30 19:37:06,122: INFO: model_logger: Epoch 9/100, train_Loss: 23403.978515625, val_Loss: 23106.6777]\n",
      "[2024-03-30 19:37:06,393: INFO: model_logger: Epoch 10/100, train_Loss: 23308.236328125, val_Loss: 22670.0078]\n",
      "[2024-03-30 19:37:06,593: INFO: model_logger: Epoch 11/100, train_Loss: 23338.78515625, val_Loss: 22888.1523]\n",
      "[2024-03-30 19:37:06,791: INFO: model_logger: Epoch 12/100, train_Loss: 23208.685546875, val_Loss: 23296.7812]\n",
      "[2024-03-30 19:37:07,057: INFO: model_logger: Epoch 13/100, train_Loss: 23206.037109375, val_Loss: 23241.1523]\n",
      "[2024-03-30 19:37:07,246: INFO: model_logger: Epoch 14/100, train_Loss: 23062.666015625, val_Loss: 23084.4023]\n",
      "[2024-03-30 19:37:07,448: INFO: model_logger: Epoch 15/100, train_Loss: 23058.849609375, val_Loss: 23610.0586]\n",
      "[2024-03-30 19:37:07,702: INFO: model_logger: Epoch 16/100, train_Loss: 23035.9453125, val_Loss: 22736.0938]\n",
      "[2024-03-30 19:37:07,903: INFO: model_logger: Epoch 17/100, train_Loss: 22952.072265625, val_Loss: 23094.4414]\n",
      "[2024-03-30 19:37:08,095: INFO: model_logger: Epoch 18/100, train_Loss: 22784.986328125, val_Loss: 22741.4824]\n",
      "[2024-03-30 19:37:08,287: INFO: model_logger: Epoch 19/100, train_Loss: 22732.857421875, val_Loss: 22559.9297]\n",
      "[2024-03-30 19:37:08,583: INFO: model_logger: Epoch 20/100, train_Loss: 22644.16796875, val_Loss: 22411.1953]\n",
      "[2024-03-30 19:37:08,776: INFO: model_logger: Epoch 21/100, train_Loss: 22548.861328125, val_Loss: 22594.1777]\n",
      "[2024-03-30 19:37:08,984: INFO: model_logger: Epoch 22/100, train_Loss: 22472.19921875, val_Loss: 22385.3574]\n",
      "[2024-03-30 19:37:09,248: INFO: model_logger: Epoch 23/100, train_Loss: 22321.703125, val_Loss: 22023.5117]\n",
      "[2024-03-30 19:37:09,443: INFO: model_logger: Epoch 24/100, train_Loss: 22220.568359375, val_Loss: 21809.6328]\n",
      "[2024-03-30 19:37:09,633: INFO: model_logger: Epoch 25/100, train_Loss: 22041.33203125, val_Loss: 21912.4512]\n",
      "[2024-03-30 19:37:09,884: INFO: model_logger: Epoch 26/100, train_Loss: 21928.328125, val_Loss: 21745.2500]\n",
      "[2024-03-30 19:37:10,085: INFO: model_logger: Epoch 27/100, train_Loss: 21818.328125, val_Loss: 21368.2285]\n",
      "[2024-03-30 19:37:10,277: INFO: model_logger: Epoch 28/100, train_Loss: 21708.544921875, val_Loss: 21504.9258]\n",
      "[2024-03-30 19:37:10,529: INFO: model_logger: Epoch 29/100, train_Loss: 21502.849609375, val_Loss: 21214.5820]\n",
      "[2024-03-30 19:37:10,719: INFO: model_logger: Epoch 30/100, train_Loss: 21306.994140625, val_Loss: 21552.1309]\n",
      "[2024-03-30 19:37:10,907: INFO: model_logger: Epoch 31/100, train_Loss: 21251.51953125, val_Loss: 21378.1602]\n",
      "[2024-03-30 19:37:11,103: INFO: model_logger: Epoch 32/100, train_Loss: 21066.431640625, val_Loss: 21441.1836]\n",
      "[2024-03-30 19:37:11,353: INFO: model_logger: Epoch 33/100, train_Loss: 20962.048828125, val_Loss: 21470.4473]\n",
      "[2024-03-30 19:37:11,547: INFO: model_logger: Epoch 34/100, train_Loss: 20740.826171875, val_Loss: 21154.4043]\n",
      "[2024-03-30 19:37:11,742: INFO: model_logger: Epoch 35/100, train_Loss: 20588.732421875, val_Loss: 20034.4844]\n",
      "[2024-03-30 19:37:11,996: INFO: model_logger: Epoch 36/100, train_Loss: 20456.205078125, val_Loss: 20507.6406]\n",
      "[2024-03-30 19:37:12,202: INFO: model_logger: Epoch 37/100, train_Loss: 20215.611328125, val_Loss: 20364.8984]\n",
      "[2024-03-30 19:37:12,403: INFO: model_logger: Epoch 38/100, train_Loss: 20104.478515625, val_Loss: 20286.9688]\n",
      "[2024-03-30 19:37:12,657: INFO: model_logger: Epoch 39/100, train_Loss: 19834.4921875, val_Loss: 19774.8574]\n",
      "[2024-03-30 19:37:12,848: INFO: model_logger: Epoch 40/100, train_Loss: 19725.521484375, val_Loss: 18802.2695]\n",
      "[2024-03-30 19:37:13,043: INFO: model_logger: Epoch 41/100, train_Loss: 19449.66015625, val_Loss: 19288.2559]\n",
      "[2024-03-30 19:37:13,237: INFO: model_logger: Epoch 42/100, train_Loss: 19366.009765625, val_Loss: 19494.9746]\n",
      "[2024-03-30 19:37:13,496: INFO: model_logger: Epoch 43/100, train_Loss: 19096.75390625, val_Loss: 19253.5449]\n",
      "[2024-03-30 19:37:13,706: INFO: model_logger: Epoch 44/100, train_Loss: 18887.666015625, val_Loss: 18949.3125]\n",
      "[2024-03-30 19:37:13,922: INFO: model_logger: Epoch 45/100, train_Loss: 18619.314453125, val_Loss: 18183.8008]\n",
      "[2024-03-30 19:37:14,219: INFO: model_logger: Epoch 46/100, train_Loss: 18398.3125, val_Loss: 18377.1152]\n",
      "[2024-03-30 19:37:14,419: INFO: model_logger: Epoch 47/100, train_Loss: 18213.6875, val_Loss: 18221.6914]\n",
      "[2024-03-30 19:37:14,611: INFO: model_logger: Epoch 48/100, train_Loss: 18049.220703125, val_Loss: 17790.1328]\n",
      "[2024-03-30 19:37:14,869: INFO: model_logger: Epoch 49/100, train_Loss: 17846.21875, val_Loss: 18067.2676]\n",
      "[2024-03-30 19:37:15,069: INFO: model_logger: Epoch 50/100, train_Loss: 17534.06640625, val_Loss: 17238.6016]\n",
      "[2024-03-30 19:37:15,278: INFO: model_logger: Epoch 51/100, train_Loss: 17417.150390625, val_Loss: 17429.3340]\n",
      "[2024-03-30 19:37:15,540: INFO: model_logger: Epoch 52/100, train_Loss: 17145.111328125, val_Loss: 17079.2188]\n",
      "[2024-03-30 19:37:15,741: INFO: model_logger: Epoch 53/100, train_Loss: 16846.416015625, val_Loss: 16579.4570]\n",
      "[2024-03-30 19:37:15,942: INFO: model_logger: Epoch 54/100, train_Loss: 16706.404296875, val_Loss: 16977.0879]\n",
      "[2024-03-30 19:37:16,140: INFO: model_logger: Epoch 55/100, train_Loss: 16459.390625, val_Loss: 16411.1855]\n",
      "[2024-03-30 19:37:16,398: INFO: model_logger: Epoch 56/100, train_Loss: 16178.408203125, val_Loss: 16528.6934]\n",
      "[2024-03-30 19:37:16,592: INFO: model_logger: Epoch 57/100, train_Loss: 15947.068359375, val_Loss: 16191.6465]\n",
      "[2024-03-30 19:37:16,782: INFO: model_logger: Epoch 58/100, train_Loss: 15700.732421875, val_Loss: 16315.5732]\n",
      "[2024-03-30 19:37:17,033: INFO: model_logger: Epoch 59/100, train_Loss: 15515.8857421875, val_Loss: 14935.1943]\n",
      "[2024-03-30 19:37:17,239: INFO: model_logger: Epoch 60/100, train_Loss: 15241.25, val_Loss: 15067.4141]\n",
      "[2024-03-30 19:37:17,453: INFO: model_logger: Epoch 61/100, train_Loss: 15061.8154296875, val_Loss: 14854.2285]\n",
      "[2024-03-30 19:37:17,722: INFO: model_logger: Epoch 62/100, train_Loss: 14790.8955078125, val_Loss: 14859.8584]\n",
      "[2024-03-30 19:37:17,923: INFO: model_logger: Epoch 63/100, train_Loss: 14572.66796875, val_Loss: 14958.9248]\n",
      "[2024-03-30 19:37:18,117: INFO: model_logger: Epoch 64/100, train_Loss: 14288.1953125, val_Loss: 13613.9941]\n",
      "[2024-03-30 19:37:18,314: INFO: model_logger: Epoch 65/100, train_Loss: 14042.7119140625, val_Loss: 14355.8398]\n",
      "[2024-03-30 19:37:18,599: INFO: model_logger: Epoch 66/100, train_Loss: 13726.85546875, val_Loss: 14409.6250]\n",
      "[2024-03-30 19:37:18,794: INFO: model_logger: Epoch 67/100, train_Loss: 13691.892578125, val_Loss: 14035.0371]\n",
      "[2024-03-30 19:37:19,030: INFO: model_logger: Epoch 68/100, train_Loss: 13291.58984375, val_Loss: 13320.0264]\n",
      "[2024-03-30 19:37:19,309: INFO: model_logger: Epoch 69/100, train_Loss: 13076.7060546875, val_Loss: 12204.8516]\n",
      "[2024-03-30 19:37:19,504: INFO: model_logger: Epoch 70/100, train_Loss: 12965.490234375, val_Loss: 13321.9219]\n",
      "[2024-03-30 19:37:19,702: INFO: model_logger: Epoch 71/100, train_Loss: 12688.078125, val_Loss: 12672.5146]\n",
      "[2024-03-30 19:37:19,954: INFO: model_logger: Epoch 72/100, train_Loss: 12451.2861328125, val_Loss: 13402.1035]\n",
      "[2024-03-30 19:37:20,142: INFO: model_logger: Epoch 73/100, train_Loss: 12303.8466796875, val_Loss: 12447.6074]\n",
      "[2024-03-30 19:37:20,332: INFO: model_logger: Epoch 74/100, train_Loss: 11991.2841796875, val_Loss: 11789.6270]\n",
      "[2024-03-30 19:37:20,582: INFO: model_logger: Epoch 75/100, train_Loss: 11830.2275390625, val_Loss: 11469.8262]\n",
      "[2024-03-30 19:37:20,781: INFO: model_logger: Epoch 76/100, train_Loss: 11681.462890625, val_Loss: 11656.9590]\n",
      "[2024-03-30 19:37:20,970: INFO: model_logger: Epoch 77/100, train_Loss: 11500.046875, val_Loss: 11983.7402]\n",
      "[2024-03-30 19:37:21,161: INFO: model_logger: Epoch 78/100, train_Loss: 11316.60546875, val_Loss: 10912.1074]\n",
      "[2024-03-30 19:37:21,412: INFO: model_logger: Epoch 79/100, train_Loss: 11225.2490234375, val_Loss: 11408.9023]\n",
      "[2024-03-30 19:37:21,607: INFO: model_logger: Epoch 80/100, train_Loss: 10997.2080078125, val_Loss: 10390.9463]\n",
      "[2024-03-30 19:37:21,802: INFO: model_logger: Epoch 81/100, train_Loss: 10755.431640625, val_Loss: 11242.3125]\n",
      "[2024-03-30 19:37:22,069: INFO: model_logger: Epoch 82/100, train_Loss: 10529.7919921875, val_Loss: 10043.1709]\n",
      "[2024-03-30 19:37:22,265: INFO: model_logger: Epoch 83/100, train_Loss: 10473.814453125, val_Loss: 10871.5762]\n",
      "[2024-03-30 19:37:22,467: INFO: model_logger: Epoch 84/100, train_Loss: 10214.685546875, val_Loss: 10078.8281]\n",
      "[2024-03-30 19:37:22,757: INFO: model_logger: Epoch 85/100, train_Loss: 10217.8984375, val_Loss: 10183.7910]\n",
      "[2024-03-30 19:37:22,963: INFO: model_logger: Epoch 86/100, train_Loss: 9963.123046875, val_Loss: 10067.5879]\n",
      "[2024-03-30 19:37:23,157: INFO: model_logger: Epoch 87/100, train_Loss: 9972.974609375, val_Loss: 9493.8408]\n",
      "[2024-03-30 19:37:23,357: INFO: model_logger: Epoch 88/100, train_Loss: 9797.7734375, val_Loss: 10010.3252]\n",
      "[2024-03-30 19:37:23,632: INFO: model_logger: Epoch 89/100, train_Loss: 9647.357421875, val_Loss: 9946.7812]\n",
      "[2024-03-30 19:37:23,842: INFO: model_logger: Epoch 90/100, train_Loss: 9506.671875, val_Loss: 9631.5781]\n",
      "[2024-03-30 19:37:24,062: INFO: model_logger: Epoch 91/100, train_Loss: 9411.59375, val_Loss: 9017.7324]\n",
      "[2024-03-30 19:37:24,326: INFO: model_logger: Epoch 92/100, train_Loss: 9312.0361328125, val_Loss: 9746.2500]\n",
      "[2024-03-30 19:37:24,527: INFO: model_logger: Epoch 93/100, train_Loss: 9057.3857421875, val_Loss: 9077.7334]\n",
      "[2024-03-30 19:37:24,735: INFO: model_logger: Epoch 94/100, train_Loss: 9230.537109375, val_Loss: 9115.8643]\n",
      "[2024-03-30 19:37:25,005: INFO: model_logger: Epoch 95/100, train_Loss: 9117.443359375, val_Loss: 9779.2422]\n",
      "[2024-03-30 19:37:25,199: INFO: model_logger: Epoch 96/100, train_Loss: 9006.7421875, val_Loss: 8844.8291]\n",
      "[2024-03-30 19:37:25,408: INFO: model_logger: Epoch 97/100, train_Loss: 8878.0859375, val_Loss: 8789.7363]\n",
      "[2024-03-30 19:37:25,670: INFO: model_logger: Epoch 98/100, train_Loss: 8967.1181640625, val_Loss: 8701.3965]\n",
      "[2024-03-30 19:37:25,863: INFO: model_logger: Epoch 99/100, train_Loss: 8890.8203125, val_Loss: 8330.6406]\n",
      "[2024-03-30 19:37:26,056: INFO: model_logger: Epoch 100/100, train_Loss: 8802.0810546875, val_Loss: 8352.1973]\n"
     ]
    }
   ],
   "source": [
    "# Model Trainer training pipeline\n",
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_trainer_config = config.get_model_trainer_config()\n",
    "    model_trainer = ModelTrainer(config=model_trainer_config)\n",
    "    model_trainer.train()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpuenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
